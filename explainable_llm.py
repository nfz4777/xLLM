# -*- coding: utf-8 -*-
"""Explainable_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XUFi6PKIlqy0K3nPEQXWXl_dmqkrsB5B
"""

import pandas as pd
import random
import os
os.environ["WANDB_DISABLED"] = "true"

# Seed for reproducibility
random.seed(42)

# Templates for text generation
attack_templates = [
    "Suspicious connection with protocol TCP, service SSH and {bytes} bytes transferred.",
    "Detected anomaly: protocol UDP, flag S0, duration {duration}s.",
    "Attack signature matched on port 21 with {bytes} bytes sent.",
    "Multiple failed login attempts from source using {service} service.",
    "High volume traffic with {bytes} bytes; likely port scan activity."
]

normal_templates = [
    "Normal web browsing session over HTTP with {bytes} bytes.",
    "Routine DNS query using UDP protocol; duration {duration}s.",
    "Successful login over SSH service with {bytes} bytes transferred.",
    "Standard email traffic detected using SMTP.",
    "Low-volume network activity recorded; no anomalies found."
]

def generate_sample(label):
    if label == 1:
        template = random.choice(attack_templates)
    else:
        template = random.choice(normal_templates)

    return {
        "text": template.format(
            bytes=random.randint(100, 10000),
            duration=random.randint(1, 60),
            service=random.choice(['FTP', 'SSH', 'Telnet', 'SMTP', 'HTTP'])
        ),
        "label": label
    }

# Generate dataset
data = [generate_sample(label) for label in ([1]*50 + [0]*50)]
random.shuffle(data)  # Shuffle to mix labels

# Convert to DataFrame
df = pd.DataFrame(data)

# Save to CSV
#df.to_csv("synthetic_network_dataset.csv", index=False)

# Show a few samples
print(df.head())

df.head()

df.shape

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
df['label'] = df['label'].astype(int)

# Train-test split
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Prepare HuggingFace Dataset
from datasets import Dataset
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(example):
    return tokenizer(example["text"], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize_function)
test_dataset = test_dataset.map(tokenize_function)

train_dataset = train_dataset.rename_column("label", "labels")
test_dataset = test_dataset.rename_column("label", "labels")

train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    #evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

sample_texts = df['text'][:5].tolist()
print(type(sample_texts))          # Should be <class 'list'>
print(all(isinstance(t, str) for t in sample_texts))  # Should be True
print(sample_texts[:3])            # Look at first few texts

import shap
from transformers import pipeline

# Create a text classification pipeline
model_name = "distilbert-base-uncased"
classifier = pipeline("text-classification", model=model_name, tokenizer=model_name, return_all_scores=True)

# Define the explainer
explainer = shap.Explainer(classifier)

# Explain a single sample
shap_values = explainer(["protocol tcp service http connection established"])

# Visualize
shap.plots.text(shap_values)

from sklearn.metrics import accuracy_score

preds = trainer.predict(test_dataset)
labels = preds.label_ids
predictions = preds.predictions.argmax(-1)

df_results = test_df.copy()
df_results['pred'] = predictions

accuracy_by_label = df_results.groupby('label').apply(lambda x: accuracy_score(x['label'], x['pred']))
print("Accuracy by label:\n", accuracy_by_label)

